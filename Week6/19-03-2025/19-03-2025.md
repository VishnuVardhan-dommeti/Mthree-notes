# Site Reliability Engineering (SRE) Concepts

## 1. Four Golden Signals

### **1.1 Latency**
- **Definition**: The time taken to serve a customer from request to completion.
- **Implementation**:
  - In our script, we measure the time taken for each web request to complete using `app_request_latency_seconds`.
  - Analogy: Like timing how long it takes from a customer ordering food until they receive it.

### **1.2 Traffic**
- **Definition**: The number of incoming requests to the system.
- **Implementation**:
  - We count incoming requests using `app_request_count`.
  - Analogy: Similar to a counter at a restaurant door that clicks each time a customer enters.

### **1.3 Errors**
- **Definition**: The number of failed requests or system errors.
- **Implementation**:
  - We track HTTP error codes (like `500` errors) and explicit error counters.
  - Analogy: Like tracking how many dishes are returned to the kitchen or how many customer complaints are received.

### **1.4 Saturation**
- **Definition**: How close the system is to reaching maximum capacity.
- **Implementation**:
  - We use `app_active_requests` to monitor how many requests are being processed simultaneously.
  - Analogy: Counting how many tables are occupied at once in a restaurant.

## 2. Observability Foundations

Observability ensures we can measure and understand system behavior.

### **2.1 Metrics**
- **Definition**: Quantitative measurements like speed, usage, and performance.
- **Implementation**:
  - Prometheus collects precise numbers on performance, such as counting requests or measuring response times.
  - Analogy: Similar to tracking altitude, speed, and fuel level in an airplane.

### **2.2 Logs**
- **Definition**: Detailed records of system events.
- **Implementation**:
  - Our application logs detailed messages with timestamps and request IDs, collected by Loki.
  - Analogy: Like a flight recorder (black box) that captures pilot actions and system events.

### **2.3 Visualization**
- **Definition**: Graphical representation of system data.
- **Implementation**:
  - Grafana creates visual dashboards to monitor system behavior.
  - Analogy: The cockpit dashboard displaying critical flight information.

### **2.4 Instrumentation**
- **Definition**: Code modifications to capture application behavior.
- **Implementation**:
  - We add instrumentation code to collect key data on request times, error rates, etc.
  - Analogy: Sensors placed throughout an aircraft to monitor various parameters.

## 3. Health Monitoring

Ensures the system is functioning correctly at different levels.

### **3.1 Liveness Probes**
- **Definition**: Checks if the application is running.
- **Implementation**:
  - The `/health/liveness` endpoint tells Kubernetes whether the application is alive.
  - Analogy: Checking if a patient has a pulse.

### **3.2 Readiness Probes**
- **Definition**: Determines if the application is ready to handle requests.
- **Implementation**:
  - The `/health/readiness` endpoint checks whether the app can handle incoming traffic properly.
  - Analogy: Checking if a patient can stand and walk before being discharged.

### **3.3 Health Endpoints**
- **Definition**: Various health checks for different system components.
- **Implementation**:
  - Different endpoints assess different aspects of system health.
  - Analogy: Medical tests like blood pressure and temperature measurements.

### **3.4 Dependency Checks**
- **Definition**: Ensures all dependencies (e.g., database, APIs) are functional.
- **Implementation**:
  - We verify that the database connection is active before marking the app as "ready".
  - Analogy: Ensuring all organs are functioning correctly before declaring a patient healthy.

## 4. Service Level Objectives (SLOs) & Performance Guarantees

Service Level Objectives (SLOs) help define reliability targets for a system.

### **4.1 Error Rate Tracking**
- **Definition**: Monitors how many requests result in errors.
- **Implementation**:
  - Tracks unsuccessful requests using HTTP status codes and custom error counters.
  - Analogy: Monitoring how many pizzas are delivered incorrectly.

### **4.2 Latency Histograms**
- **Definition**: Categorizes response times into different time buckets.
- **Implementation**:
  - Request durations are recorded in different "buckets" to analyze response time patterns.
  - Analogy: Recording delivery times across all pizza orders to understand trends.

### **4.3 Request Success Rate**
- **Definition**: Calculates the percentage of successful requests.
- **Implementation**:
  - Determines the percentage of requests that complete successfully.
  - Analogy: Calculating what percentage of pizzas are delivered correctly.

### **4.4 Defining SLOs**
- Example: **95% of pizzas will be delivered correctly within 30 minutes**
- Equivalent in our system: **99.9% of requests must complete successfully in under 500ms**.

## 5. Script Execution Steps

1. **Check Dependencies**:
   ```bash
   check_command kubectl
   check_command docker
   check_command minikube
   check_command helm
   ```
   Ensures that required tools (kubectl, docker, minikube, helm) are installed.

2. **Clean Previous Setup**:
   ```bash
   minikube delete || true
   ```
   Deletes any existing Minikube cluster to start fresh.

3. **Start Minikube**:
   ```bash
   minikube start --driver=docker --cpus=2 --memory=3072 --disk-size=10g
   ```
   Initializes a Kubernetes cluster using Minikube with optimized resource settings.

4. **Create a Flask App**:
   ```python
   app = Flask(__name__)
   ```
   Generates a lightweight Flask application with Prometheus instrumentation.

5. **Build Docker Image**:
   ```bash
   docker build -t sre-flask-app:latest .
   ```
   Builds the Flask application image inside the Minikube Docker environment.

6. **Deploy to Kubernetes**:
   ```bash
   kubectl apply -f k8s-deployment.yaml
   ```
   Applies Kubernetes configurations to deploy the application.

7. **Set Up Monitoring**:
   ```bash
   helm upgrade --install prometheus prometheus-community/prometheus --namespace monitoring
   ```
   Installs Prometheus, Loki, and Grafana using Helm.

8. **Run Load Testing**:
   ```bash
   ./load-test.sh > load-test.log 2>&1 &
   ```
   Starts a lightweight load-testing script to simulate API traffic.

9. **Expose Grafana**:
   ```bash
   kubectl port-forward svc/grafana 3000:80 -n monitoring &
   ```
   Sets up port forwarding for Grafana, allowing local access to monitoring dashboards.

10. **Final Instructions**:
    ```bash
    echo "Access Grafana at http://localhost:3000"
    ```
    Displays access URLs for the application and Grafana, along with cleanup commands.

Each step ensures smooth deployment and monitoring of an SRE-optimized Flask application within Minikube. ðŸš€
![Screenshot 2025-03-19 122749](https://github.com/user-attachments/assets/054930a0-d1c5-4b83-b381-a02d8fc661f6)


## 6. Dashboard Panels & Queries

1. **Log Volume** (Timeseries)  
   - **Query**: `sum(count_over_time({namespace="sre-demo"}[30m]))`  
   - **Explanation**: Tracks the total number of log entries in the last 30 minutes to monitor system activity.

2. **Request Count Per Endpoint** (Histogram)  
   - **Query**: `sum by(endpoint) (rate(app_request_count_total{namespace="sre-demo"}[24h]))`  
   - **Explanation**: Measures the request rate per endpoint over the past 24 hours, helping analyze traffic distribution.

3. **Request Rate Per Endpoint** (Timeseries)  
   - **Query**: `sum by(endpoint) (rate(app_request_count_total{namespace="sre-demo"}[24h]))`  
   - **Explanation**: Displays the number of requests per second grouped by endpoint to observe traffic trends.

4. **95th Percentile Latency by Endpoint** (Stat)  
   - **Query**: `histogram_quantile(0.95, sum(rate(app_request_latency_seconds_bucket{namespace="sre-demo"}[5m])) by (le, endpoint))`  
   - **Explanation**: Shows the 95th percentile latency per endpoint over 5 minutes, helping detect slow-performing services.

5. **Error Rate (%)** (Stat)  
   - **Query**: `sum(rate(app_request_count{namespace="sre-demo", http_status=~"5.."}[24h])) / sum(rate(app_request_count{namespace="sre-demo"}[24h])) * 100`  
   - **Explanation**: Calculates the percentage of requests that resulted in 5xx HTTP errors over the past 24 hours.

6. **Active Requests** (Stat)  
   - **Query**: `app_active_requests{namespace="sre-demo"}`  
   - **Explanation**: Displays the number of active requests being processed at a given time.

Each panel provides insights into system performance, reliability, and errors, making it easier to monitor and debug issues in real time. ðŸš€
![Screenshot 2025-03-19 142330](https://github.com/user-attachments/assets/20a6bac3-8e00-4f25-94ac-39670b901b32)


